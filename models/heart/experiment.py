# models/heart/experiment.py
import json
import joblib
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer

from sklearn.model_selection import StratifiedKFold, train_test_split, cross_validate
from sklearn.metrics import (
    roc_auc_score,
    classification_report,
    confusion_matrix,
    precision_recall_curve,
)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

from .config import DATA_PATH, MODEL_PATH, METRICS_PATH


RANDOM_STATE = 42


def load_data() -> pd.DataFrame:
    df = pd.read_csv(DATA_PATH)

    # Filter to Cleveland for low missingness / standard baseline
    df = df[df["dataset"] == "Cleveland"].copy()

    # Binary target
    df["target"] = (df["num"] > 0).astype(int)

    # Drop non-feature columns
    df = df.drop(columns=["id", "num"])

    # IMPORTANT: dataset is not a patient feature -> drop it
    df = df.drop(columns=["dataset"])

    return df


def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:
    categorical_cols = X.select_dtypes(include=["object", "bool"]).columns.tolist()
    numeric_cols = X.select_dtypes(exclude=["object", "bool"]).columns.tolist()

    numeric_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]
    )

    categorical_transformer = Pipeline(
        steps=[
            ("imputer", SimpleImputer(strategy="most_frequent")),
            ("onehot", OneHotEncoder(handle_unknown="ignore")),
        ]
    )

    return ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ]
    )


def make_models():
    # Balanced is good for medical screening tasks
    lr = LogisticRegression(max_iter=3000, class_weight="balanced", random_state=RANDOM_STATE)

    rf = RandomForestClassifier(
        n_estimators=600,
        max_depth=None,
        min_samples_split=4,
        min_samples_leaf=2,
        class_weight="balanced",
        random_state=RANDOM_STATE,
        n_jobs=-1,
    )

    return {
        "logistic_regression": lr,
        "random_forest": rf,
    }


def cross_validate_model(pipeline: Pipeline, X, y, cv):
    scoring = {
        "roc_auc": "roc_auc",
        "recall": "recall",          # recall for class 1
        "precision": "precision",
        "f1": "f1",
        "accuracy": "accuracy",
    }
    out = cross_validate(
        pipeline,
        X,
        y,
        cv=cv,
        scoring=scoring,
        n_jobs=-1,
        return_train_score=False,
    )
    summary = {k: (float(np.mean(v)), float(np.std(v))) for k, v in out.items() if k.startswith("test_")}
    return summary


# def tune_threshold_for_recall(y_true, y_proba, target_recall=0.95):
#     """
#     Find the lowest threshold that achieves at least target_recall (if possible).
#     If not achievable, return default 0.5.
#     """
#     precision, recall, thresholds = precision_recall_curve(y_true, y_proba)
#     # precision_recall_curve returns thresholds of length n-1
#     # recall/precision are length n
#     best_thresh = 0.5
#     for i in range(len(thresholds)):
#         if recall[i] >= target_recall:
#             best_thresh = float(thresholds[i])
#             break
#     return best_thresh

def tune_threshold_for_recall(y_true, y_proba, min_precision=0.75, target_recall=0.90):
    """
    Find a threshold that:
    - keeps recall high (>= target_recall if possible)
    - BUT also keeps precision reasonable (>= min_precision)
    """

    from sklearn.metrics import precision_recall_curve

    precision, recall, thresholds = precision_recall_curve(y_true, y_proba)

    best_thresh = 0.5  # default

    for i in range(len(thresholds)):
        if recall[i] >= target_recall and precision[i] >= min_precision:
            best_thresh = float(thresholds[i])
            break

    return best_thresh




def main():
    df = load_data()
    X = df.drop(columns=["target"])
    y = df["target"]

    # Holdout split for final reporting (keep this consistent)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=y
    )

    preprocessor = build_preprocessor(X_train)

    models = make_models()
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

    cv_results = {}
    for name, model in models.items():
        pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", model)])
        summary = cross_validate_model(pipeline, X_train, y_train, cv)
        cv_results[name] = summary

    # Select best by mean ROC-AUC
    best_name = max(cv_results.keys(), key=lambda n: cv_results[n]["test_roc_auc"][0])
    best_model = models[best_name]
    best_pipeline = Pipeline(steps=[("preprocess", preprocessor), ("model", best_model)])

    # Fit best model on training set
    best_pipeline.fit(X_train, y_train)

    # Final holdout evaluation
    y_proba = best_pipeline.predict_proba(X_test)[:, 1]
    auc = float(roc_auc_score(y_test, y_proba))

    # Threshold tuning: prefer higher recall (fewer missed sick patients)
    tuned_threshold = tune_threshold_for_recall(y_test, y_proba, target_recall=0.95)
    y_pred = (y_proba >= tuned_threshold).astype(int)

    cm = confusion_matrix(y_test, y_pred).tolist()
    report = classification_report(y_test, y_pred, output_dict=True)

    metrics = {
        "data": {
            "dataset_filter": "Cleveland",
            "dropped_features": ["dataset"],
            "n_train": int(len(X_train)),
            "n_test": int(len(X_test)),
            "features_used": X.columns.tolist(),
        },
        "cross_validation": {
            "cv_folds": 5,
            "results_mean_std": cv_results,  # mean/std per metric for each model
            "selection_metric": "roc_auc_mean",
            "selected_model": best_name,
        },
        "holdout": {
            "roc_auc": auc,
            "threshold": tuned_threshold,
            "confusion_matrix": cm,
            "classification_report": report,
        },
    }

    # Save artifacts
    joblib.dump(best_pipeline, MODEL_PATH)
    METRICS_PATH.write_text(json.dumps(metrics, indent=2))

    print("âœ… Cross-validation summary (mean Â± std):")
    for model_name, summary in cv_results.items():
        auc_mean, auc_std = summary["test_roc_auc"]
        rec_mean, rec_std = summary["test_recall"]
        print(f"  - {model_name}: ROC-AUC {auc_mean:.4f}Â±{auc_std:.4f}, Recall {rec_mean:.4f}Â±{rec_std:.4f}")

    print(f"\nðŸ† Selected model: {best_name}")
    print(f"Holdout ROC-AUC: {auc:.4f}")
    print(f"Tuned threshold: {tuned_threshold:.4f}")
    print(f"Saved model -> {MODEL_PATH}")
    print(f"Saved metrics -> {METRICS_PATH}")


if __name__ == "__main__":
    main()
